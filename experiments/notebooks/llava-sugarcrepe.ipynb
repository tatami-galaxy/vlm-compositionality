{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94329964c2e4866ac3de4c3dd664e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4972c866e94559a0c1ec72a0027c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6074d173a4f407fb1ec4718c79c6615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ade0dc47d69461a846e86c64852acfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04266ee8444867adc43d5aa249118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee45c71b09874a1388187b7e6a8a3363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dc3737de1348ebaab44c1693cd7eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a860174dcb3467f9045e89dd7449bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cf40a17c504d6eb6340d02e691625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6707dcbb824c4a8fba6c999ecf7a0325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2242222b33a4cc29960bab01348f799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e16fa01f82499db8136fe3e7b62f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506e677ef5004b709da2d3a6c5d8c217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4591de0672bf443285b504e057aa6361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed1265939ca47b5acaa7b9e61799cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c2078ec2d54580a360e19e2c5c38ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "# model_name\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# load model\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "     attn_implementation=\"flash_attention_2\",\n",
    "     device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What are these?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <image>\\nWhat are these? ASSISTANT:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"USER: <image>\\nWhat are these? ASSISTANT:\"\n",
    "#prompt = \"USER: What are these?\\n<image> ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "What are these? ASSISTANT: These are two cats lying on a pink couch.\n"
     ]
    }
   ],
   "source": [
    "image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/root/vlm-compositionality/data/raw/sugarcrepe/'\n",
    "\n",
    "# add attribute\n",
    "with open (folder_path+'add_att.json') as f:\n",
    "    add_attribute = json.load(f)\n",
    "\n",
    "# add object\n",
    "with open (folder_path+'add_obj.json') as f:\n",
    "    add_object = json.load(f)\n",
    "\n",
    "# replace attribute\n",
    "with open (folder_path+'replace_att.json') as f:\n",
    "    replace_attribute = json.load(f)\n",
    "\n",
    "# replace object\n",
    "with open (folder_path+'replace_obj.json') as f:\n",
    "    replace_object = json.load(f)\n",
    "\n",
    "# replace relation\n",
    "with open (folder_path+'replace_rel.json') as f:\n",
    "    replace_relation = json.load(f)\n",
    "\n",
    "# swap attribute\n",
    "with open (folder_path+'swap_att.json') as f:\n",
    "    swap_attribute = json.load(f)\n",
    "\n",
    "# swap object\n",
    "with open (folder_path+'swap_obj.json') as f:\n",
    "    swap_object = json.load(f)\n",
    "\n",
    "# collate together\n",
    "dataset = {\n",
    "    'add_attribute': add_attribute, 'add_object': add_object, 'replace_attribute': replace_attribute,\n",
    "    'replace_object': replace_object, 'replace_relation': replace_relation,\n",
    "    'swap_attribute': swap_attribute, 'swap_object': swap_object,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = '/root/vlm-compositionality/data/raw/coco_val_2017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '000000021503.jpg',\n",
       " 'caption': 'Crackers coated with spread, sitting on a plate, ready to eat.',\n",
       " 'negative_caption': 'Spread coated with crackers, sitting on a plate, ready to eat.'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = '67'\n",
    "img_file = swap_object[id]['filename']\n",
    "caption = swap_object[id]['caption']\n",
    "negative_caption = swap_object[id]['negative_caption']\n",
    "swap_object[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat = [\n",
    "\n",
    "  #{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "\n",
    "  #{\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "\n",
    "  #{\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <image>\\nChoose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: Men wash a motorcycle while girls look on. Caption 2: Girls wash a motorcycle while men look on. ASSISTANT: Caption 2 USER: <image>\\nChoose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: The stop sign is behind the fence instead of on the street. Caption 2: The fence is behind the stop sign instead of on the street. ASSISTANT: Caption 1 USER: <image>\\nChoose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: A girl with a ball facing a man with a racquet. Caption 2: A man with a ball facing a girl with a racquet. ASSISTANT:'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: Men wash a motorcycle while girls look on. Caption 2: Girls wash a motorcycle while men look on.\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Caption 2\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: The stop sign is behind the fence instead of on the street. Caption 2: The fence is behind the stop sign instead of on the street.\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Caption 1\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": (\"Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: {} Caption 2: {}\").format(negative_caption, caption)},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    img_folder+'000000177934.jpg',\n",
    "    img_folder+'000000011122.jpg',\n",
    "    img_folder+img_file,\n",
    "]\n",
    "    \n",
    "raw_images = [Image.open(image) for image in images]\n",
    "inputs = processor(images=raw_images, text=text, return_tensors='pt').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: Men wash a motorcycle while girls look on. Caption 2: Girls wash a motorcycle while men look on. ASSISTANT: Caption 2 USER:  \n",
      "Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: The stop sign is behind the fence instead of on the street. Caption 2: The fence is behind the stop sign instead of on the street. ASSISTANT: Caption 1 USER:  \n",
      "Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: A girl with a ball facing a man with a racquet. Caption 2: A man with a ball facing a girl with a racquet. ASSISTANT: Caption 1 USER: A man with a ball facing a girl with a racquet.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <image>\\nChoose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: A couple men pat an elephant as a woman watches. Caption 2: A woman pats an elephant as a couple men watch. ASSISTANT:'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": (\"Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: {} Caption 2: {}\").format(negative_caption, caption)},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    img_folder+img_file,\n",
    "]\n",
    "    \n",
    "raw_images = [Image.open(image) for image in images]\n",
    "inputs = processor(images=raw_images, text=text, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "Choose and return the correct caption for the image from the following 2 captions. Generate no other text. Caption 1: A couple men pat an elephant as a woman watches. Caption 2: A woman pats an elephant as a couple men watch. ASSISTANT: Caption 1: A couple men pat an elephant as a woman watches.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <image>\\nSelect the correct caption and only the caption number for the given image. The captions are as follows : Caption 1: Spread coated with crackers, sitting on a plate, ready to eat. Caption 2: Crackers coated with spread, sitting on a plate, ready to eat. ASSISTANT:'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": (\"Select the correct caption and only the caption number for the given image. The captions are as follows : Caption 1: {} Caption 2: {}\").format(negative_caption, caption)},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER:  \n",
      "Select the correct caption and only the caption number for the given image. The captions are as follows : Caption 1: Spread coated with crackers, sitting on a plate, ready to eat. Caption 2: Crackers coated with spread, sitting on a plate, ready to eat. ASSISTANT: Caption 1\n"
     ]
    }
   ],
   "source": [
    "images = [\n",
    "    img_folder+img_file,\n",
    "]\n",
    "    \n",
    "raw_images = [Image.open(image) for image in images]\n",
    "inputs = processor(images=raw_images, text=text, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
