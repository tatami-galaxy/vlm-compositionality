{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26cd0dc-8642-40da-aec7-35c87e7eb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, Qwen2VLForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef4e882-bd8d-4fd8-ab5a-b7ec8d492ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/media/drdo/DATA/LLMs/Molmo-7B-O-0924'\n",
    "checkpoint = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244cb559-1ecb-40e8-9924-cbc4bdfaff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1153938-3283-4236-9235-291c4fb32a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e444a4106da74e178fde1a9028106a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype='auto',\n",
    "        device_map='auto'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192aa90b-d64d-4c5e-be7c-fa1665d4742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/323 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=checkpoint,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f779816a-66e8-4add-b15e-ef0b80b10132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200143f8-e0af-401b-b3ed-d3694e09119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the image and text\n",
    "inputs = processor.process(\n",
    "    images=[Image.open(requests.get(\"https://picsum.photos/id/237/536/354\", stream=True).raw)],\n",
    "    text=\"Describe this image.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbbd025-f17e-48ac-8682-9f8106d9f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move inputs to the correct device and make a batch of size 1\n",
    "inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f749e703-4345-4163-a948-4182c25cc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allenai/Molmo-7B-O-0924 -> 9min 5s\n",
    "# allenai/MolmoE-1B-0924 -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "734e6c76-4ed7-4f97-9002-5ebb1afa7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 39s, sys: 25.7 s, total: 9min 5s\n",
      "Wall time: 10min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\n",
    "output = model.generate_from_batch(\n",
    "    inputs,\n",
    "    GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "    tokenizer=processor.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ad289f-3bbb-4fcb-ae2d-a6651d5c98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a detailed photograph of a small black puppy, likely a Labrador, sitting on a weathered wooden deck. The deck's planks are light brown with dark brown and black streaks, and the wood appears to be aged and worn. The puppy is positioned in the center of the image, with its body facing the camera and its head slightly tilted to the right. Its ears are floppy and hang down on either side of its head. The puppy's eyes are a striking brown, and it has a shiny black nose. The puppy's front paws are visible, with one paw slightly extended forward. The background is simple, consisting solely of the wooden deck, which emphasizes the puppy as the focal point of the image. The photograph is taken from a top-down perspective, capturing the puppy's endearing and curious expression as it looks up at the camera.\n"
     ]
    }
   ],
   "source": [
    "# only get generated tokens; decode them to text\n",
    "generated_tokens = output[0,inputs['input_ids'].size(1):]\n",
    "generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# print the generated text\n",
    "print(generated_text)\n",
    "\n",
    "# >>> This photograph captures an adorable black Labrador puppy sitting on a weathered\n",
    "#     wooden deck. The deck's planks, which are a mix of light and dark brown with ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490bba4-aa80-454b-bf87-86042f96649b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417442a7-7645-41a7-b1f7-1cffea8a05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import CONFIG\n",
    "\n",
    "CONFIG.set_default_api_key(\"nnsight_api_key\") #nnsight_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1bf06df-bfb2-42ca-a7fc-6c09a0d624e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# llama3.1 70b is a gated model and you need access via your huggingface token\n",
    "os.environ['HF_TOKEN'] = \"hf_key\" #hf_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369166b-5701-4b34-a605-242ea19ae875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 10:23:51,079 bb8408ee-be73-462c-9762-4801073caed1 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-10-24 10:23:51,145 bb8408ee-be73-462c-9762-4801073caed1 - APPROVED: Your job was approved and is waiting to be run.\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "# We'll never actually load the parameters so no need to specify a device_map.\n",
    "llama = LanguageModel(\"meta-llama/Meta-Llama-3.1-70B\")\n",
    "\n",
    "# All we need to specify using NDIF vs executing locally is remote=True.\n",
    "with llama.trace(\"The Eiffel Tower is in the city of\", remote=True) as runner:\n",
    "    hidden_states = llama.model.layers[-1].output.save()\n",
    "    output = llama.output.save()\n",
    "\n",
    "print(hidden_states)\n",
    "print(output[\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c327e73-2bea-4a0d-b7f4-79ebc3ea12fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09fae7e2-bf37-470c-b5eb-491ced0e783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311dd1daf60e43a1b360f0a0a2c07d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The image depicts a serene beach scene with a woman and a dog. The woman is sitting on the sand, wearing a plaid shirt and black pants, and appears to be smiling. She is holding the dog's paw in a high-five gesture. The dog, which is a large breed, is sitting on the sand with its front paws raised, possibly in response to the woman's gesture. The background shows the ocean with gentle waves, and the sky is clear with a soft light, suggesting it might be either sunrise or sunset. The overall atmosphere is peaceful and joyful.\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_name = 'Qwen2-VL-2B-Instruct'\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458cf74-cd45-40b9-bece-5c2b75c1626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
