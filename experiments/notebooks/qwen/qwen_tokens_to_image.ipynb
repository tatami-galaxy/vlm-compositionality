{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db04ce8-080c-4b57-9ee1-243be36b744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import io\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2901d9-e93a-4d6a-9515-378ed4e8f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2a375cbe9d4ff88493def735831a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model in half-precision on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36fec5e5-b16e-41e6-a59d-745a41244057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _preprocess in Qwen2VLImageProcessor\n",
    "# image can be visualized till `patches`\n",
    "# data_format == ChannelDimension.LAST -> False\n",
    "# patches.shape[0] % self.temporal_patch_size != 0 -> True\n",
    "# TODO: why does processed_image look different with imshow?\n",
    "\n",
    "# patches -> 2, 3, 1372, 2044 (image repeated across dim=0, c, h, w)\n",
    "# grid_t = 1, grid_h = 98, grid_w = 146\n",
    "# temporal_patch_size = 2\n",
    "# patch_size = 14\n",
    "# merge_size = 2\n",
    "# resized_height = 1372\n",
    "# resized_width = 2044\n",
    "# channel = 3\n",
    "\n",
    "#processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2636dff-705e-400e-a096-134bc5466e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches -> (1, 3, 1372, 2044)\n",
    "\n",
    "#if patches.shape[0] % self.temporal_patch_size != 0:\n",
    "    #repeats = np.repeat(patches[-1][np.newaxis], self.temporal_patch_size - 1, axis=0)\n",
    "    #patches = np.concatenate([patches, repeats], axis=0)\n",
    "\n",
    "# patches -> (2, 3, 1372, 2044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5bdb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 49, 2, 14, 73, 2, 14)\n",
      "(1, 49, 73, 2, 2, 3, 2, 14, 14)\n",
      "(14308, 1176)\n"
     ]
    }
   ],
   "source": [
    "height = 1372\n",
    "width = 2044\n",
    "channel = 3\n",
    "temporal_patch_size = 2\n",
    "patch_size = 14\n",
    "merge_size = 2\n",
    "patches = np.random.rand(2, channel, height, width)\n",
    "\n",
    "grid_t = patches.shape[0] // temporal_patch_size\n",
    "grid_h = height // patch_size\n",
    "grid_w = width // patch_size\n",
    "\n",
    "# patches -> (2, 3, 1372, 2044)\n",
    "\n",
    "patches = patches.reshape(\n",
    "    grid_t,\n",
    "    temporal_patch_size,\n",
    "    channel,\n",
    "    grid_h // merge_size,\n",
    "    merge_size,\n",
    "    patch_size,\n",
    "    grid_w // merge_size,\n",
    "    merge_size,\n",
    "    patch_size,\n",
    ")\n",
    "print(patches.shape)\n",
    "patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
    "print(patches.shape)\n",
    "flatten_patches = patches.reshape(\n",
    "    grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n",
    ")\n",
    "print(flatten_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_embeds in Qwen2VLForConditionalGeneration -> 3577, 3884\n",
    "# (image_grid_thw[index].prod() // merge_length) = 3577\n",
    "\n",
    "# TODO:\n",
    "# 3577 image tokens (same as image_emebds) -> how to get from flatten_patches (14308, 1176)?\n",
    "# pixel_values fed into image_embeds in Qwen2VisionTransformerPretrainedModel\n",
    "# map image_tokens (3577, 3884) -> image_embeds (3577, 3884)\n",
    "# -> pixel_values (14308, 1176) -> flatten_patches (14308, 1176)\n",
    "# -> image patches (2, 3, 1372, 2044) -> original image (2, c, h, w)\n",
    "# can print (c, h, w)\n",
    "\n",
    "# map image_embeds (3577, 3884) to image (2, 3, 1372, 2044) :\n",
    "\n",
    "# map pixel_values (14308, 1176) to image (2, 3, 1372, 2044)\n",
    "# visualize which part of pixel_values correspond to which part of image\n",
    "\n",
    "# map image_embeds (3577, 3884) to pixel_values (14308, 1176)\n",
    "\n",
    "# hidden states have 3577 image token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d82f60-3e3c-4312-b3d0-f38ff05f59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llava image inputs -> channel x height x wdith (3, 336, 336)\n",
    "# image to embeddings :\n",
    "# image size = (336, 336)\n",
    "# kernel size = (14, 14) (stride = 14)\n",
    "# (336 * 336) / (14 * 14) = 576 embeddings\n",
    "# softmax(embeddings) -> interpolate and plot over image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a5fe5-ce70-4457-84d0-795f0223893a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeeb6f7f-605e-4fed-be73-0ef40aba1c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|image_pad|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3a3d99-4ec8-452d-8a08-c8ff1678e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1372, 2044)\n",
      "(2, 3, 1372, 2044)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m text_prompt \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(text_prompt)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#raise\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py:117\u001b[0m, in \u001b[0;36mQwen2VLProcessor.__call__\u001b[0;34m(self, images, text, videos, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m output_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_kwargs(\n\u001b[1;32m    112\u001b[0m     Qwen2VLProcessorKwargs,\n\u001b[1;32m    113\u001b[0m     tokenizer_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39minit_kwargs,\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     image_grid_thw \u001b[38;5;241m=\u001b[39m image_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_grid_thw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:425\u001b[0m, in \u001b[0;36mQwen2VLImageProcessor.preprocess\u001b[0;34m(self, images, videos, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    423\u001b[0m pixel_values, vision_grid_thws \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m--> 425\u001b[0m     patches, image_grid_thw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m     pixel_values\u001b[38;5;241m.\u001b[39mextend(patches)\n\u001b[1;32m    439\u001b[0m     vision_grid_thws\u001b[38;5;241m.\u001b[39mappend(image_grid_thw)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:302\u001b[0m, in \u001b[0;36mQwen2VLImageProcessor._preprocess\u001b[0;34m(self, images, do_resize, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    299\u001b[0m     patches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([patches, repeats], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mprint\u001b[39m(patches\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    304\u001b[0m channel \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    305\u001b[0m grid_t \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_patch_size\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "# Image\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "#print(text_prompt)\n",
    "#raise\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a6b362-834f-4d4a-9ef0-f08d3ec94b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab4316-2371-4668-b041-201d1e6320e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_grid_thw -> tensor([[  1,  98, 146]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea34e0a3-0532-4259-a2e5-0f266fdfe9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14308)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[  1,  98, 146]])\n",
    "a[0].prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04da5de9-41fe-4c46-9cc8-705b3533f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14308"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[  1,  98, 146]])\n",
    "a.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508b29f0-aadc-4bf3-9f34-11f310bc27e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VisionTransformerPretrainedModel(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "  )\n",
       "  (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x Qwen2VLVisionBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): VisionFlashAttention2(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp): VisionMlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): QuickGELUActivation()\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merger): PatchMerger(\n",
       "    (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b23ab-a076-41bc-b7ff-c2cc935c678b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f73285fe-0d93-4bef-a69c-4dd244750a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3577, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero((inputs['input_ids'][0] == 151655).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a95095aa-c3b8-47c7-a056-419fe1e53506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0][50] == 151655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ba8219-5e69-449e-8ac6-824a388429d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16464"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1176*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf35da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
