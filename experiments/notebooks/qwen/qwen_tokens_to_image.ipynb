{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db04ce8-080c-4b57-9ee1-243be36b744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import io\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import math\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2901d9-e93a-4d6a-9515-378ed4e8f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2195ac68999546868b01124563d3ffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model in half-precision on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd224209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4968b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68c0253a",
   "metadata": {},
   "source": [
    "##### Example image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03233e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers.image_utils import(\n",
    "    make_flat_list_of_images,\n",
    "    make_list_of_images,\n",
    "    to_numpy_array,\n",
    "    infer_channel_dimension_format,\n",
    "    get_image_size,\n",
    "    ChannelDimension,\n",
    ")\n",
    "\n",
    "from transformers.image_transforms import resize, rescale, normalize, to_channel_dimension_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4378679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8acde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d19b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cfcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_resize(\n",
    "    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n",
    "):\n",
    "    \"\"\"Rescales the image so that the following conditions are met:\n",
    "\n",
    "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
    "\n",
    "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
    "\n",
    "    3. The aspect ratio of the image is maintained as closely as possible.\n",
    "\n",
    "    \"\"\"\n",
    "    if height < factor or width < factor:\n",
    "        raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n",
    "    elif max(height, width) / min(height, width) > 200:\n",
    "        raise ValueError(\n",
    "            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n",
    "        )\n",
    "    h_bar = round(height / factor) * factor\n",
    "    w_bar = round(width / factor) * factor\n",
    "    if h_bar * w_bar > max_pixels:\n",
    "        beta = math.sqrt((height * width) / max_pixels)\n",
    "        h_bar = math.floor(height / beta / factor) * factor\n",
    "        w_bar = math.floor(width / beta / factor) * factor\n",
    "    elif h_bar * w_bar < min_pixels:\n",
    "        beta = math.sqrt(min_pixels / (height * width))\n",
    "        h_bar = math.ceil(height * beta / factor) * factor\n",
    "        w_bar = math.ceil(width * beta / factor) * factor\n",
    "    return h_bar, w_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff2829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14308, 1176)\n"
     ]
    }
   ],
   "source": [
    "max_pixels = 12845056\n",
    "merge_size = 2\n",
    "min_pixels = 3136\n",
    "patch_size = 14\n",
    "resample = 3\n",
    "rescale_factor = 0.00392156862745098\n",
    "temporal_patch_size = 2\n",
    "\n",
    "images = make_flat_list_of_images(image_inputs)\n",
    "images = make_list_of_images(images[0])\n",
    "images = [to_numpy_array(image) for image in images]\n",
    "input_data_format = infer_channel_dimension_format(images[0])\n",
    "\n",
    "height, width = get_image_size(images[0], channel_dim=input_data_format)\n",
    "resized_height, resized_width = height, width\n",
    "\n",
    "processed_images = []\n",
    "for image in images:\n",
    "    resized_height, resized_width = smart_resize(\n",
    "        height,\n",
    "        width,\n",
    "        factor=patch_size * merge_size,\n",
    "        min_pixels=min_pixels,\n",
    "        max_pixels=max_pixels,\n",
    "    )\n",
    "    image = resize(\n",
    "        image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n",
    "    )\n",
    "    image = rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n",
    "    image = to_channel_dimension_format(image, ChannelDimension.FIRST, input_channel_dim=input_data_format)\n",
    "    processed_images.append(image)\n",
    "    \n",
    "patches = np.array(processed_images)\n",
    "\n",
    "if patches.shape[0] % temporal_patch_size != 0:\n",
    "    repeats = np.repeat(patches[-1][np.newaxis], temporal_patch_size - 1, axis=0)\n",
    "    patches = np.concatenate([patches, repeats], axis=0)\n",
    "\n",
    "#print(patches.shape)\n",
    "#plt.imshow(patches[1].transpose(1,2,0))\n",
    "#raise\n",
    "\n",
    "channel = patches.shape[1]\n",
    "grid_t = patches.shape[0] // temporal_patch_size\n",
    "grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n",
    "patches = patches.reshape(\n",
    "    grid_t,\n",
    "    temporal_patch_size,\n",
    "    channel,\n",
    "    grid_h // merge_size,\n",
    "    merge_size,\n",
    "    patch_size,\n",
    "    grid_w // merge_size,\n",
    "    merge_size,\n",
    "    patch_size,\n",
    ")\n",
    "patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
    "\n",
    "# TODO: map to patches\n",
    "# patches -> (2, 3, 1372, 2044)\n",
    "# 1372*2044 == (98*14) * (146*14)\n",
    "# grid_t = 1, grid_h = 98, grid_w = 146\n",
    "# patch_size = 14\n",
    "# grid_h * grid_w = 14308, channel * temporal_patch_size * patch_size * patch_size = 1176\n",
    "flatten_patches = patches.reshape(\n",
    "    grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n",
    ")\n",
    "\n",
    "print(flatten_patches.shape)\n",
    "\n",
    "## unravel_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d30eb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(14308 *1176) / (1372*2044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b274d690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches -> 2, 3, 1372, 2044 (image repeated across dim=0, c, h, w)\n",
    "# grid_t = 1, grid_h = 98, grid_w = 146\n",
    "# temporal_patch_size = 2\n",
    "# patch_size = 14\n",
    "# merge_size = 2\n",
    "# resized_height = 1372\n",
    "# resized_width = 2044\n",
    "# channel = 3\n",
    "\n",
    "# image_embeds in Qwen2VLForConditionalGeneration -> 3577, 3884\n",
    "# (image_grid_thw[index].prod() // merge_length) = 3577\n",
    "\n",
    "# pixel_values fed into image_embeds in Qwen2VisionTransformerPretrainedModel\n",
    "# original image (2, c, h, w)\n",
    "# can print (c, h, w)\n",
    "\n",
    "# image (2, 3, 1372, 2044) -> image_embeds (14308, 1176)\n",
    "# image_embeds (14308, 1176) -> patch_embed (Conv3d) -> image_embeds (14308, 1280) -> patchmerger \n",
    "# patchmerger:\n",
    "# dim = 3584, context_dim = 1280, spatial_merge_size = 2\n",
    "# hidden_size = context_dim * (spatial_merge_size**2) = 5120\n",
    "# patchmerger: image_embeds.view(-1, 5120) -> image_embeds (3577, 5120) \n",
    "# image_embeds (3577, 5120) -> linear -> image_embeds (3577, 3584) \n",
    "\n",
    "# TODO: map pixel_values (14308, 1176) to patches (2, 3, 1372, 2044)\n",
    "# (grid_t * grid_h * grid_w) = 14308, (channel * temporal_patch_size * patch_size * patch_size) = 1176\n",
    "\n",
    "# how to preserve mapping across transformations?\n",
    "\n",
    "# hidden states have 3577 image token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b15002-0bec-4502-9ff6-0df8539a4d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1372*2044 == (98*14) * (146*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf617be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1372*2044*6 == 14308*1176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03c61e75-e15b-4de7-8d3a-1ca63cfd1f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14308 * 1280 == 3577 * 5120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0a337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1272dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92b17365-08d9-4c49-af58-f14ff8406c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "im1 = np.arange(100).reshape((10, 10))\n",
    "im2 = im1.T\n",
    "im3 = np.flipud(im1)\n",
    "im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(4., 4.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 2),  # creates 2x2 grid of Axes\n",
    "                 axes_pad=0.0,  # pad between Axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [im1, im2, im3, im4]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ef7b76-1f55-40eb-b007-3f26587067e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 14, 14, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (grid_t * grid_h * grid_w) = 14308, (channel * temporal_patch_size * patch_size * patch_size) = 1176\n",
    "\n",
    "a = np.random.rand(14308, 1176)\n",
    "a[0].reshape((2,3,14,-1)).transpose(0,2,3,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd568050-a1de-4cca-a3ef-eebafeca92e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d82f60-3e3c-4312-b3d0-f38ff05f59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llava image inputs -> channel x height x wdith (3, 336, 336)\n",
    "# image to embeddings :\n",
    "# image size = (336, 336)\n",
    "# kernel size = (14, 14) (stride = 14)\n",
    "# (336 * 336) / (14 * 14) = 576 embeddings\n",
    "# softmax(embeddings) -> interpolate and plot over image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a5fe5-ce70-4457-84d0-795f0223893a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeeb6f7f-605e-4fed-be73-0ef40aba1c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|image_pad|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3a3d99-4ec8-452d-8a08-c8ff1678e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "#print(text_prompt)\n",
    "#raise\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a6b362-834f-4d4a-9ef0-f08d3ec94b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab4316-2371-4668-b041-201d1e6320e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_grid_thw -> tensor([[  1,  98, 146]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea34e0a3-0532-4259-a2e5-0f266fdfe9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14308)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[  1,  98, 146]])\n",
    "a[0].prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04da5de9-41fe-4c46-9cc8-705b3533f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14308"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[  1,  98, 146]])\n",
    "a.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508b29f0-aadc-4bf3-9f34-11f310bc27e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VisionTransformerPretrainedModel(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "  )\n",
       "  (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x Qwen2VLVisionBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): VisionFlashAttention2(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp): VisionMlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): QuickGELUActivation()\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merger): PatchMerger(\n",
       "    (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "039b23ab-a076-41bc-b7ff-c2cc935c678b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14308, 1176])\n",
      "torch.Size([14308, 1280])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Inference: Generation of the output\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids) :]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, output_ids)\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m      8\u001b[0m     generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3231\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3228\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3231\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3232\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1649\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1648\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mget_dtype())\n\u001b[0;32m-> 1649\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1650\u001b[0m     n_image_tokens \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1651\u001b[0m     n_image_features \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:998\u001b[0m, in \u001b[0;36mQwen2VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m    996\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(hidden_states)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_states\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m rotary_pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrot_pos_emb(grid_thw)\n\u001b[1;32m   1002\u001b[0m cu_seqlens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(grid_thw[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m grid_thw[:, \u001b[38;5;241m2\u001b[39m], grid_thw[:, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcumsum(\n\u001b[1;32m   1003\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;66;03m# Select dtype based on the following factors:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mgrid_thw\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mint32,\n\u001b[1;32m   1009\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bcb394-e91c-48eb-9453-fbffffe597a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3577, 5120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(14308, 1280)\n",
    "a = a.view(-1, 5120)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34689fab-e59c-47a3-8552-4920ae1a409c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f73285fe-0d93-4bef-a69c-4dd244750a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3577, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero((inputs['input_ids'][0] == 151655).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a95095aa-c3b8-47c7-a056-419fe1e53506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0][50] == 151655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ba8219-5e69-449e-8ac6-824a388429d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16464"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1176*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcf35da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionFlashAttention2(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLFlashAttention2(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f35a0-6132-4428-af75-d55b70b7f3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
